{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject finder using SpaCy\n",
    "\n",
    "When exploring a long text, or a series of texts with overlapping characters (like *Star Wars* novels), it may be useful to look at the verbs that co-occur with characters. What are the characters in your text *doing*, and how do those actions differ between characters?\n",
    "\n",
    "Ideally, you'd want to do some [coreference resolution](https://towardsdatascience.com/most-popular-coreference-resolution-frameworks-574ba8a8cc2d) to be able to link all the pronouns in the text to the character they refer to. Unfortunately, there's no easy or reliable way to go about doing it. Only using character names, and ignoring pronouns, is much more feasible.\n",
    "\n",
    "This code takes a folder of plain-text (.txt) files and creates, in the same directory as that folder, a CSV file with each subject, verb, and the name of the text file it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Download SpaCy and language model\n",
    "If you don't already have the SpaCy library installed, the code below downloads it along with the English language model. This notebook should work for any [language with a SpaCy model](https://spacy.io/models). Just substitute the name of the model for *en_core_web_sm* (For instance, if you wanted to use Lithuanian, you can replace `en_core_web_sm` with `lt_core_news_sm`). Places where you need to do this substitution are commented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace en_core_web_sm with another model name here for other languages\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import modules\n",
    "To use a [SpaCy language model](https://spacy.io/models) other than English, replace `en_core_web_sm` with the model name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os is used for navigating directories\n",
    "import os\n",
    "# spacy is used for identifying the subjects and verbs\n",
    "import spacy\n",
    "#Replace en_core_web_sm with another model name here for other languages\n",
    "import en_core_web_sm\n",
    "#Replace en_core_web_sm with another model name here for other languages\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('She really liked Karen and Andrew, and she couldn\\'t believe Kristy is doing this.')\n",
    "#doc = nlp('Mary Anne Looks at Kristy in disbelief')\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Select directory\n",
    "Edit the. code cell below to put in the full path to the directory/folder with the text files (.txt) you want to work with. Note that this notebook will process *all* text files in the directory you specify, so you may need to put just the files you're interested in into their own directory.\n",
    "\n",
    "If you have only one text file (e.g. a single novel) and aren't comfortable reworking the code to run on only one file, you can create a directory and put the text file inside it.\n",
    "\n",
    "Here's the syntax to specify the full path to a folder called *YOUR-FOLDER* within the Documents directory:\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents/YOUR-FOLDER'\n",
    "* On Windows: 'C:\\\\Users\\\\YOUR-USER-NAME\\\\Documents\\\\YOUR-FOLDER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/qad/Documents/dhtrek/dhtrek_textcorpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes the notebook's working directory to the directory you specified\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Find subjects\n",
    "The code cells below reads each text file in the directory, finds every subject and verb, and writes it to a CSV file along with the filename it's from. The files are processed alphabetically, so that if something breaks (e.g. text files bigger than 1 MB may trigger a memory error), you can figure out what files are left to be done.\n",
    "\n",
    "By default, the CSV file is called `charverbs.csv` and gets created inside the directory with the text files. You can give the file a different name in the code cell below, but keeping `.csv` is recommended.\n",
    "\n",
    "Doing the NLP parse can be time-consuming, particularly if you have a large number of files. If your text is on the scale of hundreds of novels, expect it to take hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charverbfile = 'trek-verbs-lemma.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens the output file\n",
    "with open(charverbfile, 'w') as out:\n",
    "    out.write('Filename, Subject, Verb' + '\\n')\n",
    "    #Sorts the files alphabetically\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        #Looks for .txt files\n",
    "        if filename.endswith('.txt'):\n",
    "            #Opens each file\n",
    "            with open(filename, 'r') as bookfile:\n",
    "                #Reads in the text in the file\n",
    "                book = bookfile.read()\n",
    "                #NLP parse of the text\n",
    "                doc = nlp(book)\n",
    "                #Noun chunks are the part of the SpaCy dependency parse that we need\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    #If the dependency relation is 'nsubj' (noun subject)\n",
    "                    if chunk.root.dep_ == 'nsubj':\n",
    "                        #Write the filename, the noun chunk, the verb, and then a newline character\n",
    "                        strsubj = str(chunk.text)\n",
    "                        cleansubj = strsubj.replace(',', '')                        \n",
    "                        out.write(filename + ', ' + cleansubj + ', ' + chunk.root.head.text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude\n",
    "Now you have a text file with a lot of subjects and verbs -- most of which you may not be able to use (because they involve a pronoun that you can't easily resolve to a character, or don't refer to a character at all). \n",
    "\n",
    "You may want to just explore this file in an interface that makes it easier to read with your eyeballs. You could load it into [OpenRefine](https://openrefine.org/) and use the *Text Filter* function on the Subject or Verb column to see what verbs are associated with a character, or which characters are associated with a verb. (If your corpus is small enough, you might be able to use *Text Facet* on the verb column to see what all the verbs are.) This can be a useful step to surface potential problems you might encounter as you run the next set of code; for instance, you could discover that searching for the character name \"Mara\" (with no space at the end) also gets you \"Marauders\" as a subject.\n",
    "\n",
    "What I did next was create a copy of the `charverbs.csv` file, and removed all rows with a pronoun in them, using [regular expressions](https://programminghistorian.org/en/lessons/understanding-regular-expressions) in a plain text editor. (E.g. to get rid of \"she\" as a pronoun, I searched for `^.*, she(.*)` and replaced those lines with nothing.)\n",
    "\n",
    "Once I got rid of all the pronouns, I went back to Python with the resulting CSV (saved in the folder of source texts, just like the original output CSV) to look for the most common subjects (which should largely align with the main characters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Load CSV into a dataframe\n",
    "Pandas dataframes are basically the spreadsheets of Python. They offer numerous options for creating complex subsets from large amounts of tabular data. The code cells below import the pandas module and create a dataframe from the CSV where you've removed the pronouns.\n",
    "\n",
    "The code below assumes that you've run the code cells above, and that your working directory is the same directory you specified earlier. If you've named your clean CSV something other than `charverbs-clean.csv`, specify it in the second code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charverbs = pd.read_csv('charverbs-clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charverbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
